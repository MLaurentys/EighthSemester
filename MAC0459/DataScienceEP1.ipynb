{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification.\n",
    "\n",
    "Name: Matheus T. de Laurentys\n",
    "\n",
    "NUSP: 9793714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name of the dataset\n",
    "Dataset: Women's Shoe Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Abstract of the dataset (describe the dataset with your own words):\n",
    "\n",
    "The Women's Shoe Prices dataset is a collection that lists sevral different shoe models and some information about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to the dataset\n",
    "~~Question 1: What are the shoe categories with the largest seasonal(witer/summer/...) differences in price?~~\n",
    "(Shoe Categories do not represent what we expected)\n",
    "\n",
    "Question 1: What is the percentage of shoes which the prices changed?\n",
    "\n",
    "~~Question 2: What is the percentage of sellers that sell more than 60% of their years sellings in a single season?~~\n",
    "(Merchants Data listed on the pdf is not present in this dataset)\n",
    "\n",
    "Question 2: Did the shoe size mean/std_dev change with time? \n",
    "\n",
    "Question 3: What are the favorite colors for each month of the year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Present your EDA strategy:\n",
    "1. Checked which columns represented categories instead of values\n",
    "1. From the data columns made a trnalation unity if needed (example: 6 BM US -> size 6, medium width)\n",
    "1. I \"fixed\" some of the data that has \"non-uniform\" data so I can later annalyze it (size)\n",
    "1. I organize the data I will focus in categorical vs numerical so I can easily treat them\n",
    "1. I followed the order suggested by the list below (leaving the item 5 last)\n",
    "\n",
    "For the coding part:\n",
    "* I left dictionaries, enums and auxiliriary methods in the beggining to ease the readability later on\n",
    "* I do not keep much unnecessary data available, but that can be easily changed, and have the values verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of analysis/visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Present the mean, variance, min, median and max values for each attribute. If the dataset has too many attibutes, choose the more significant ones. Present your code and some remarks to help the understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) How well distributed are the dataset in relation the each attribute. How can you visually check that? Hint: plot a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Is there any attribute that can be used to better interpret the dataset? Show this in a graphic where you used this attribute to group (groupby) the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Is there any outlier's pattern? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Make your own remarks about the dataset. Try to use one or more graphics to justify your remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Is there any symmetry that can be stressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) How important was EDA to help you understand the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing DataSet, Helper Code and Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "from enum import Enum\n",
    "\n",
    "data_raw_19 = pd.read_csv(\"Datafiniti_Womens_Shoes_Jun19.csv\")\n",
    "data_raw_1 = pd.read_csv(\"7210_1.csv\", low_memory=False)\n",
    "\n",
    "aux = ['0','1','2','3','4','5','6','7','8','9']\n",
    "aux2 = ['0','1','2','3','4','5','6','7','8','9', '.']\n",
    "def extract_size(s):\n",
    "    a = list(s)\n",
    "    b = []\n",
    "    l = len(a)\n",
    "    for i in range(l):\n",
    "        if (a[i] in aux):\n",
    "            while(i < l and a[i] in aux2):\n",
    "                b.append(a[i])\n",
    "                i += 1\n",
    "            f = ''.join(b)\n",
    "            return float(f)\n",
    "        \n",
    "        \n",
    "def barplot(df, col_name):\n",
    "    name = \"up to the 10 most significants \" + col_name\n",
    "    ax = df[col_name].value_counts().head(10).plot(kind='bar',\n",
    "                                    figsize=(30,8),\n",
    "                                    title=name)\n",
    "def distPlot(df, col_name):\n",
    "    name = \"Distribution of \" + col_name\n",
    "    ax = df.groupby('id').first()[col_name].plot.kde(title=name)\n",
    "    if(col_name == 'prices.size'): ax.set_xlim(-1.0, 20.0)\n",
    "    else: ax.set_xlim(-5.0, 1500.0)\n",
    "\n",
    "        \n",
    "data_fix = data_raw_19\n",
    "data_fix['prices.size'] = data_fix['prices.size'].apply(lambda x : extract_size(x))\n",
    "\n",
    "class col_raw (Enum):\n",
    "    ids = 0\n",
    "    brand = 4\n",
    "    categories = 5\n",
    "    primaryCategories = 6\n",
    "    colors = 7\n",
    "    p_amt_max = 15\n",
    "    p_amt_min = 16\n",
    "    p_colors = 18\n",
    "    p_size = 28\n",
    "    #sizes = 30\n",
    "    \n",
    "numericals = [\n",
    "    col_raw.p_amt_max.value,\n",
    "    col_raw.p_amt_min.value,\n",
    "    col_raw.p_size.value\n",
    "    #col_raw.sizes.value\n",
    "]\n",
    "\n",
    "categorical = [\n",
    "    col_raw.brand.value,\n",
    "    col_raw.primaryCategories.value,\n",
    "    col_raw.categories.value,\n",
    "    col_raw.p_colors.value,\n",
    "    col_raw.colors.value\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean, variance, min, median and max values for each \"important\" attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Values\n",
    "1. Prices.amountMax\n",
    "2. Prices.amountMin\n",
    "4. Sizes\n",
    "\n",
    "#### Categorical Values\n",
    "1. Brand\n",
    "2. primaryCategories\n",
    "3. Colors\n",
    "4. Categories\n",
    "5. Prices.colors\n",
    "\n",
    "#### Ignored Entries\n",
    "* Ean\n",
    "* Asins\n",
    "* ID\n",
    "* Date Added\n",
    "* Date Updated\n",
    "* imageURLs\n",
    "* Keys\n",
    "* manufacturer (86% is null)\n",
    "* manufacturerNumber (86% is null)\n",
    "* name\n",
    "* prices.condition\n",
    "* Prices.size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_data(df, col_index):\n",
    "    col_name = df.columns[col_index]\n",
    "    vals = df[col_name].unique()\n",
    "    return col_name, vals\n",
    "\n",
    "\n",
    "    \n",
    "def numerical_data(df, col_index):\n",
    "    col_name = df.columns[col_index]\n",
    "    des = df[col_name].describe()\n",
    "    minV = des.min()\n",
    "    maxV = des.max()\n",
    "    mean =  des.mean()\n",
    "    median =  des.median()\n",
    "    variance =  des.std()**2\n",
    "    return mean, variance, median, minV, maxV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in numericals:\n",
    "    mean, var, median, minV, maxV = numerical_data(data_fix, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check data distribution in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in categorical:\n",
    "    name, vals = categorical_data(data_fix, item)\n",
    "    barplot(data_fix, name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in numericals:\n",
    "    col_name = data_fix.columns[item]\n",
    "    distPlot(data_fix, col_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annalysis\n",
    "\n",
    "As you can see in every category there is a very significant top1 and top2. Even thought data is more distributed on the different brands, there is still a very high concentration, as there are 10,000 entries and 1,600 are from only two brands.\n",
    "* I considered size as a real variable and not a category for this annalysis, because its categories are, in fact, directly related to the values they convey.\n",
    "\n",
    "The pices do show us there is not focus on shoes that are costly. I think the range found does say that standard shoes were selected and I could not make up any bias from that data, as the distribution soes match what I expected, roughly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Most relevante attribute to the understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute that really helped understand what the data actually is, surprinsingly enough, was the distribution by brands. This information made me realize that the data concentrated information of standardized shoes (with names, even). Before I thought the focus would be in a few categories where brands would play lesse role than dates and seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected the biggest difference from reality to be in the prices, but, to  my surprise, that is not the case. I could not find anything special, but, I the colors sold the most surprised me (not enough to call doubt the sample). I tought I would see much more brown and gray than white or black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. My remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I did ignore information about the shoe format, and filtered the soze,exclusively\n",
    "* When plotting the categories, I fixed an upper limit of 10 because it started getting difficult to read at 15, so I rounded to a confortable level (10)\n",
    "* I did not filter repeated entries when plotting the brands, because I believe the repeated data is imortant to eunderstand the focus of the set\n",
    "* I did filter repeated entries when plotting the numerical distributions, because to see the distribution I thought an estimate of the item price would suffice. The estimation I did was to simply get the first value on the table, not the mean. I chose this because I think it has a similar result to other methods, and I do not know which one is best\n",
    "* I ignored entries I did not know how to work with, entries that were mostly blank, as they would not show much of the whole sample, or repeated columns (sizes vs. p_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data_fix.plot(x='prices.amountMin', y='prices.amountMax', style='.')\n",
    "ax.set_xlim(-5.0, 1000.0)\n",
    "ax.set_ylim(-5.0, 1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows a very clear positive correlation between the maximum and minimum price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data_fix.plot(x='prices.amountMin', y='prices.size', style='.')\n",
    "ax.set_xlim(-5.0, 1000.0)\n",
    "ax.set_ylim(-5.0, 20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, however, shows that there is no clear correlation between sizes and the cost. It shows, however, other symmetrical information. The price range of small and large shoes is smaller than the size of regular sized shoes. It does make sense, as there is way fewer demand for this sizes, and, as a consequence, much less offer, so there is a smaller price sample. What is not clear is why the price mean is smaller in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Importance of EDA to help me understand the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am very new to data science and this was the first time I spent some hours with a dataset exploring how to handle and compare data. With all honesty, I know there are amazing things and interesting relations that can be found/done. I did not, however, expect it would make a great difference on how I would handle my dataset. I thought it would be something way over my head.\n",
    "\n",
    "To my surprise, I did find a way to use the things I learned I learned in class. More specifically I had a straight forward approach to this assignment, where I \"knew\" what I would find. The data, however, did not match what I expected. A few different examples: I expected to get interesting data about manufacturers, but, mostly the data did not exist; I never expected to get anyhting special from brands, but data was, indeed, concentrated on a small set of brands; I though the focus would be on the different categories (flip-flops, sandals, heels, sneakers...), however, the categories meant something completely different.\n",
    "\n",
    "I would say that EDA helped me refocus on what the data conveyed, and not what I wanted it to bring me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to the dataset\n",
    "\n",
    "Question 1: What is the percentage of shoes which the prices changed?\n",
    "\n",
    "Question 2: What is the number of shoes which there is difference between the minimum and maximum prices? \n",
    "\n",
    "Question 3: What are the months entries count throughout the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = len(data_fix.index)\n",
    "s2 = len(data_fix.groupby(\"id\"))\n",
    "\n",
    "p = 100 * (1.0 - float(s2)/float(s1))\n",
    "print(\"percentage = {0}%\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data_fix.loc[data_fix[\"prices.amountMax\"] - data_fix[\"prices.amountMin\"] > 0.0]\n",
    "len(df2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0 for i in range(12)]\n",
    "for i in range(12):\n",
    "    b = str(i+1)\n",
    "    if (len(b) == 1): b = '0' + b\n",
    "    a[i] = data_fix.loc[data_fix[\"dateAdded\"].str[5:7] == b]\n",
    "c = [0 for i in range(12)]\n",
    "for i in range(12):\n",
    "    c[i] = len(a[i].index)\n",
    "c.sort()\n",
    "for i in range(12):\n",
    "    print(\"Month = {0}, Count = {1}\".format(i+1, c[i]))\n",
    "x = np.arange(12)\n",
    "plt.bar(x, c)\n",
    "plt.xticks(x, ('Jan', 'Fev', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe my questions show that the sample is very biased.\n",
    "1. Almost all the shoe models were introduced in December\n",
    "2. All the shoes have no difference between their minimum and maximum price"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "134.4px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
